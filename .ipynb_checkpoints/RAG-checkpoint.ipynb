{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114fffa4-1951-40d0-8bdf-3f848e4522ff",
   "metadata": {},
   "source": [
    "# ✅ **What is RAG**\n",
    "\n",
    "---\n",
    "\n",
    "**RAG** stands for **Retrieval-Augmented Generation**.  \n",
    "It combines **LLMs** with **external knowledge sources** to generate more accurate and grounded responses.\n",
    "\n",
    " **Purpose**:  \n",
    "Retrieve relevant information from a knowledge base and feed it to the LLM for context-aware generation.\n",
    "\n",
    "---\n",
    "\n",
    "# Components of RAG\n",
    "\n",
    "RAG typically uses **four key components**:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Document Loaders\n",
    "\n",
    "- **Function**: Load data from various sources (PDFs, websites, Notion, files, etc.)\n",
    "- **Purpose**: Convert raw, unstructured content into a structured list of `Document` objects used in LangChain pipelines.\n",
    "\n",
    "- **Examples**:\n",
    "\n",
    "- `PyPDFLoader` – Loads content from PDF files  \n",
    "- `WebBaseLoader` – Loads content from websites  \n",
    "- `NotionLoader` – Loads content from Notion pages  \n",
    "- `TextLoader` – Loads plain `.txt` files  \n",
    "- `DirectoryLoader` – Loads all documents from a directory (can combine with any loader type)  \n",
    "- `CSVLoader` – Loads data from CSV files (as documents per row or column)  \n",
    "- **Custom Document Loader** – You can subclass `BaseLoader` to define your own loader for any data source (e.g., internal tools, APIs)\n",
    "\n",
    "---\n",
    "\n",
    "| **Glob Pattern** | **What It Loads**                                          |\n",
    "| ---------------- | ---------------------------------------------------------- |\n",
    "| `\"**/*.txt\"`     | All `.txt` files in all folders and subfolders (recursive) |\n",
    "| `\"*.pdf\"`        | All `.pdf` files in the root directory only                |\n",
    "| `\"data/*.csv\"`   | All `.csv` files inside the `data/` folder (not recursive) |\n",
    "| `\"**/*\"`         | All files of any type in all folders and subfolders        |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Text Splitters\n",
    "- **Function**: Split large texts into smaller, manageable chunks.\n",
    "- **Purpose**: Improve chunk-level retrieval accuracy and avoid context overflow.\n",
    "\n",
    "- **Examples**:\n",
    "| **Splitter Type**        | **What It Does**                      | **Examples**                                            |\n",
    "| ------------------------ | ------------------------------------- | ------------------------------------------------------- |\n",
    "| Length-Based             | Splits by size (tokens/chars)         | Fixed-size, overlapping, recursive character splitters  |\n",
    "| Text Structure-Based     | Uses natural text layout              | Sentence-based, paragraph-based, custom delimiter split |\n",
    "| Document Structure-Based | Uses formal layout structure          | Headings in Markdown, PDF sections, slide breaks        |\n",
    "| Semantic Meaning-Based   | Splits where topic or meaning changes | Embedding-based splits, semantic chunkers               |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Vector Databases\n",
    "- **Function**: Store text chunks as **embeddings** (vector representations).\n",
    "- **Purpose**: Enable efficient **similarity search** based on vector distances.\n",
    "- **Examples**:\n",
    "  - FAISS  \n",
    "  - Pinecone  \n",
    "  - Chroma  \n",
    "  - Weaviate  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Retrievers\n",
    "- **Function**: Query the vector store to find the most relevant chunks for a given input.\n",
    "- **Purpose**: Supply useful context to the LLM from stored documents.\n",
    "- **Examples**:\n",
    "  - Similarity-based search  \n",
    "  - Top-k result selection  \n",
    "  - Optional metadata filtering\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **`load()` vs `lazy_load()`**\n",
    "\n",
    "- `load()`  \n",
    "  - **Eager loading**  \n",
    "  - Reads and processes **all documents at once**  \n",
    "  - Returns a list of `Document` objects  \n",
    "  - Good for small/medium datasets\n",
    "\n",
    "- `lazy_load()`  \n",
    "  - **Lazy (streamed) loading**  \n",
    "  - Returns a **generator** instead of a list  \n",
    "  - ✅ Memory efficient for large datasets  \n",
    "  - Useful when you want to process documents one-by-one or in batches\n",
    "\n",
    "```python\n",
    "# Example\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"notes.txt\")\n",
    "docs = loader.load()         # Loads all documents immediately\n",
    "docs_lazy = loader.lazy_load()  # Loads documents one-by-one as needed\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d62fd-463c-491d-9212-280d72bcb556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
